<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Revisiting Multi-Model LLM Evaluations"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG"/>
  <meta property="og:url" content="MLLM_Evaluations"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="Revisiting Multi-Model LLM Evaluations">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Revisiting Multi-Model LLM Evaluations</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Revisiting Multi-Model LLM Evaluations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="/index.html" target="_blank">Jian Lu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://nik-dim.github.io" target="_blank">Shikhar Srivastava</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://gortizi.github.io/" target="_blank">Junyu Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://fleuret.org/" target="_blank">Robik Shrestha</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.epfl.ch/labs/lts4/people/people-current/frossard/" target="_blank">Manoj Acharya</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.epfl.ch/labs/lts4/people/people-current/frossard/" target="_blank">Kushal Kafle</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.epfl.ch/labs/lts4/people/people-current/frossard/" target="_blank">Christopher Kanan</a><sup>3</sup>
            </span>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup></sup>University of Rochester
                <!--<br>Neurips Workshop 2024</span>-->
              <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author: jlu59@u.rochester.edu</small></span>
            </div>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/KevinLuJian/MLLMs_webpage" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Introduction -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstractions</h2>
        <div class="content has-text-justified">
          <p>With the advent of multi-modal large language models (MLLMs), datasets used for visual question answering (VQA) and referring expression comprehension have seen a resurgence. However, the most popular datasets used to evaluate MLLMs are some of the earliest ones created, and they have many known problems, including extreme bias, spurious correlations, and an inability to permit fine-grained analysis. In this paper, we pioneer evaluating recent MLLMs (LLaVA 1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed to address weaknesses in earlier ones.  We assess three VQA datasets: 1) TDIUC, which permits fine-grained analysis on 12 question types, 2) TallyQA, which has simple and complex counting questions, and 3) DVQA, which requires optical character recognition for chart understanding. We also study VQDv1, a dataset that requires identifying all image regions that satisfy a given query. Our experiments reveal the weaknesses of many MLLMs that have not previously been reported. Our code is integrated into the widely used LAVIS framework for MLLM evaluation, enabling the rapid assessment of future MLLMs.</p>
          <p>VQDv1, TallyQA, and DVQA evaluations are not based on complete datasets but instead use representative samples. This effectively shortens the testing sets, allowing developers to expedite the evaluation process while maintaining the diversity of the original datasets. This website provides the datasets, evaluation scripts, and example answer files for researchers to reproduce our results.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Datasets -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Datasets</h2>

    <h3 class="title is-4">VQDv1</h3>
    <p align="center">
      <img src="https://raw.githubusercontent.com/KevinLuJian/MLLMs_webpage/main/VQDv1.png" alt="Example Image"/>
    </p>
    <p>VQDv1 requires the model to produce multiple bounding boxes instead of localizing only one object, thereby testing general query detection skills. Unlike typical referring expression datasets, which assert that every query will correspond to only one bounding box, VQDv1 queries ask the model to generate an uncertain number of bounding boxes, from 0 to N, posing an additional challenge to the model.</p>

    <h3 class="title is-4">TallyQA</h3>
    <p align="center">
      <img src="https://raw.githubusercontent.com/KevinLuJian/MLLMs_webpage/main/Images/TallyQA/CleanShot%202024-06-16%20at%2015.07.48%402x.png" alt="Example Image"/>
    </p>
    <p>TallyQA tests models' visual grounding through counting skills. In addition to simple counting questions that the model can handle well with straightforward object detection, TallyQA also incorporates complex counting questions that demand sophisticated reasoning capabilities, such as pose estimation (e.g., "How many dogs are sitting?") and positional reasoning (e.g., "How many dogs are in front of the white building?").</p>

    <h3 class="title is-4">TDIUC</h3>
    <p align="center">
      <img src="static/images/TDIUC.png" alt="Example Image"/>
    </p>
    <p>TDIUC tests the models' versatility across 12 tasks, including object, attribute, and activity recognition, as well as overall scene understanding. The meaningful categories of question types permit fine-grain analysis of the models' abilities from different perspectives, allowing us to identify the specific strengths and weaknesses of each model.</p>


    <h3 class="title is-4">DVQA</h3>
    <p align="center">
      <img src="https://raw.githubusercontent.com/KevinLuJian/MLLMs_webpage/main/Images/DVQA/CleanShot%202024-06-16%20at%2014.32.32%402x.png" alt="Example Image"/>
    </p>
    <p>DVQA requires the models to interpret and analyze visual data in chart form, testing their ability to perform OCR and handle unusual words found in charts. The charts are all synthetically generated images, which pose additional challenges different from natural images.</p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Evaluation Metrics</h2>
    <div class="content has-text-justified">
      <p>Evaluating the performance of models within a multi-modal framework requires the use of various metrics that reflect distinct aspects of model capabilities. Accurately understanding these metrics is essential for a precise interpretation of results and for assessing the strengths and weaknesses across different models. In this evaluation, we focus on two types of accuracy: micro accuracy and macro accuracy.</p>
      
      <ul>
        <li><strong>Micro Accuracy:</strong> This metric treats each question equally, representing the traditional approach. It is calculated by dividing the number of correctly answered questions by the total number of questions. This provides a straightforward measure of overall performance across all queries.</li>
        <li><strong>Macro Accuracy:</strong> Also referred to as Mean-Per-Type accuracy and termed Macro Accuracy for consistency in this paper, this metric ensures that each type of question contributes equally to the overall accuracy. By calculating the arithmetic mean of accuracy across different question types, this measure prevents models from inflating their overall scores by excelling in frequent question types while performing poorly on less common ones. This approach provides a balanced evaluation, highlighting models' capabilities across varied tasks.</li>
      </ul>
    </div>
  </div>
</section>

<!-- Main Results Section -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Main Results</h2>

    <!-- VQDv1 Results -->
    <h3 class="title is-4">VQDv1 Results</h3>
    <p align="center">
      <img src="static/images/results/VQDv1_result.png" alt="VQDv1 Result Image" style="margin-bottom: 10px;">
      <figcaption>VQDv1 Result Overview</figcaption>
      <img src="static/images/results/VQDv1_graph.png" alt="VQDv1 Performance Graph" style="margin-top: 10px;">
    </p>
    <p>All models exhibit declined Recall when the number of bounding boxes increases, implying that these models demonstrate an inability to identify multiple objects that satisfy the query (e.g., "show me the sink," some models may only be able to identify one, but not others). 
      This trend suggests that increasing the number of grounding objects challenges the models' detection capabilities, potentially due to limitations in how they interpret contextual information within visual scenes.</p>
    </p>

    <!-- TallyQA Results -->
    <h3 class="title is-4">TallyQA Results</h3>
    <p align="center">
      <img src="static/images/results/TallyQA_result.png" alt="TallyQA Result Image"/>
    </p>
    <p>
      TallyQA composes of two parts, simple counting questions and complex counting questions. The results show that the some models that perform well on simple counting questions may struggle with complex counting questions.
      For instance, BLIP2 have a comparable performance with other more complex models, such as GPT-4, but it performance drop significantly in complex counting questions, which requires, in additional to simple object detection, more sophisticated reasoning capabilities,
      such as pose estimation and positional reasoning. This suggests the necessity of incorporating more complex counting questions in the evaluation of visual reasoning models.
    </p>
    <p align="center">
      <img src="static/images/results/TallyQA_detail.png" alt="TallyQA Result Image"/>
    </p>
    <p>The testing metrics of TallyQA is somewhat allign with VQDv1, the ability to count multiply objects the foundations for 
      grounding multiple objects in the image. Fig 8 and 9 shows that most model demonstrate performance decay wehn counting multiply objects.
      This is consistent with the results in VQDv1, where model generally perform less well on multiply object detections. 
    </p>

    <!-- TDIUC Results -->
    <h3 class="title is-4">TDIUC Results</h3>
    <p align="center">
      <img src="static/images/results/TDIUC_result.png" alt="TDIUC Result Image"/>
    </p>
    <p>
      The results in TDIUC provide us the detail performance of each model on different question types. 
      The results show that the models have different strengths and weaknesses on different question types. 
      A few key takeaway, all models demonstrate inability in positional reasoning. LLaVANext has the highest performance in almost all question types, even surpassing
      previous SoTA VQA algorithm MuRel.
    </p>

    <!-- DVQA Results -->
    <h3 class="title is-4">DVQA Results</h3>
    <p align="center">
      <img src="static/images/results/DVQA_result.png" alt="DVQA Result Image"/>
    </p>
    <p>
      The results for DVQA shows that some model, while perform well on datasets with natural images, may struggle with synthetic images. The DVQA datasets are all synthetic images. The bars are generated
      with random color and styles, with many abbreivations involves. Most models are espicially struggle in reasoning and data retrieval quesitons. 
    </p>
  </div>
</section>


<!-- Key Takeway
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Key takeway</h2>
    <p>For each dataset, we provide example answer files in the format of the question-answer pairs we used. The example answer files can be found at:</p>
    <ul>
      <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/Evaluation_result(Ours)">Download example answer files</a></li>
    </ul>
    <p>The most important components are the predicted answer and the labeled answer.</p>
  </div>
</section> -->


<!-- License -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">License</h2>
    <p>&copy; 2024 Multi-Model LLM Evaluations. All rights reserved.</p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. You are free to borrow the structure of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

</body>
</html>
