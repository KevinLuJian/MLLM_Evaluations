<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Revisiting Multi-Model LLM Evaluations"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="Revisiting Multi-Model LLM Evaluations">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Revisiting Multi-Model LLM Evaluations</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Revisiting Multi-Model LLM Evaluations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj Acharya, Kushal Kafle, Christopher Kanan</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/KevinLuJian/MLLM-evaluation" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Introduction -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>In this paper, we evaluate popular multi-modal large language models (MLLMs) on four datasets created by our lab: VQDv1, TallyQA, TDIUC, and DVQA. These datasets are designed to assess the performance of multi-modal LLMs on various tasks, including visual question answering, counting, and image understanding.</p>
          <p>VQDv1, TallyQA, and DVQA evaluations are not based on complete datasets but instead use representative samples. This effectively shortens the testing sets, allowing developers to expedite the evaluation process while maintaining the diversity of the original datasets. This website provides the datasets, evaluation scripts, and example answer files for researchers to reproduce our results.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Datasets -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Datasets</h2>

    <h3 class="title is-4">VQDv1</h3>
    <p align="center">
      <img src="https://github.com/KevinLuJian/MLLMs_webpage/blob/main/VQDv1.png" alt="Example Image"/>
    </p>
    <p>VQDv1 requires the model to produce multiple bounding boxes instead of localizing only one object, thereby testing general query detection skills. Unlike typical referring expression datasets, which assert that every query will correspond to only one bounding box, VQDv1 queries ask the model to generate an uncertain number of bounding boxes, from 0 to N, posing an additional challenge to the model.</p>
    <ul>
      <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/VQDv1_sampling.json">Download VQDv1 question-answer pairs</a></li>
      <li><a href="http://images.cocodataset.org/zips/val2014.zip">Download VQDv1 images (val2014)</a></li>
    </ul>

    <h3 class="title is-4">TallyQA</h3>
    <p align="center">
      <img src="https://github.com/KevinLuJian/MLLMs_webpage/blob/main/Images/TallyQA/CleanShot%202024-06-16%20at%2015.07.48%402x.png" alt="Example Image"/>
    </p>
    <p>TallyQA tests models' visual grounding through counting skills. In addition to simple counting questions that the model can handle well with straightforward object detection, TallyQA also incorporates complex counting questions that demand sophisticated reasoning capabilities, such as pose estimation (e.g., "How many dogs are sitting?") and positional reasoning (e.g., "How many dogs are in front of the white building?").</p>
    <ul>
      <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/TallyQA_test.json">Download TallyQA testing dataset</a></li>
      <li><a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip">Download TallyQA images part 1</a></li>
      <li><a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip">Download TallyQA images part 2</a></li>
    </ul>

    <h3 class="title is-4">TDIUC</h3>
    <p align="center">
      <img src="https://github.com/KevinLuJian/MLLMs_webpage/blob/04f42c895e05ebea41071c2ebc38448bae83aa34/Images/TDIUC.png" alt="Example Image"/>
    </p>
    <p>TDIUC tests the models' versatility across 12 tasks, including object, attribute, and activity recognition, as well as overall scene understanding. The meaningful categories of question types permit fine-grain analysis of the models' abilities from different perspectives, allowing us to identify the specific strengths and weaknesses of each model.</p>
    <ul>
      <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/TDIUC_sampling.json">Download TDIUC question-answer pairs</a></li>
      <li><a href="https://drive.google.com/file/d/1Hevf7eQNzg-qlXbfz9nPbATmQciexkDp/view?usp=share_link">Download TDIUC images</a></li>
    </ul>

    <h3 class="title is-4">DVQA</h3>
    <p align="center">
      <img src="https://github.com/KevinLuJian/MLLMs_webpage/blob/main/Images/DVQA/CleanShot%202024-06-16%20at%2014.32.32%402x.png" alt="Example Image"/>
    </p>
    <p>DVQA requires the models to interpret and analyze visual data in chart form, testing their ability to perform OCR and handle unusual words found in charts. The charts are all synthetically generated images, which pose additional challenges different from natural images.</p>
    <ul>
      <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/raw/main/DVQA_sampling.json">Download DVQA question-answer pairs</a></li>
      <li><a href="https://drive.google.com/file/d/1iOSjgbqnTiLpMFuuRa3kIs3E_RxGkKmX/view?usp=share_link">Download DVQA images</a></li>
    </ul>
  </div>
</section>

<!-- Evaluation Script -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation Script</h2>
        <div class="content has-text-justified">
          <p>To evaluate the performance of the models on the datasets, we provide evaluation scripts for each dataset. Please prepare the answer files in the format of the question-answer pairs we provided. You can download the evaluation scripts from this repository:</p>
          <ul>
            <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/eval_script">Download evaluation scripts</a></li>
          </ul>
          <p>Detailed instructions on how to use the scripts are available in the repository.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Example Answer Files -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Example Answer Files</h2>
    <p>For each dataset, we provide example answer files in the format of the question-answer pairs we used. The example answer files can be found at:</p>
    <ul>
      <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/Evaluation_result(Ours)">Download example answer files</a></li>
    </ul>
    <p>The most important components are the predicted answer and the labeled answer.</p>
  </div>
</section>

<!-- Data Sampling Script -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data Sampling Script</h2>
        <div class="content has-text-justified">
          <p>For the datasets VQDv1, TDIUC, and DVQA, where we sample a portion of the original testing datasets, we provide the sampling scripts that show how the datasets are being sampled, with detailed instructions in the readme file.</p>
          <ul>
            <li><a href="https://github.com/KevinLuJian/MLLM-evaluation/tree/main/datasets_sampling">Download data sampling scripts</a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- License -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">License</h2>
    <p>&copy; 2024 Multi-Model LLM Evaluations. All rights reserved.</p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. You are free to borrow the structure of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

</body>
</html>
