<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Revisiting Multi-Modal LLM Evaluations"/>
  <meta property="og:description" content="Revisiting Multi-Modal LLM Evaluations"/>
  <meta property="og:url" content="https://kevinlujian.github.io/MLLM_Evaluations"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner/banner_output_high_res.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  
  <meta name="twitter:title" content="Revisiting Multi-Modal LLM Evaluations">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="static/images/University_of_Rochester_seal.png">
  <meta name="twitter:card" content="static/images/University_of_Rochester_seal.png">
  
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="multi-modal LLM, LLM, evaluation, tallqa, dvqa, tduic, vqdv1">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Revisiting Multi-Modal LLM Evaluations</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">


  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link href="https://fonts.googleapis.com/css2?family=Latin+Modern+Roman:wght@400;700&display=swap" rel="stylesheet">
  <style>
    h2.title {
      font-family: 'Latin Modern Roman', serif;
      font-weight: 700;
      font-size: 2.5rem; /* Adjust the size as needed */
      line-height: 1.2;
    }
  </style>
    <style>
      h1.title {
        font-family: 'Latin Modern Roman', serif;
        font-weight: 700;
        font-size: 2.5rem; /* Adjust the size as needed */
        line-height: 1.2;
      }
    </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Revisiting Multi-Modal LLM Evaluations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="/index.html" target="_blank">Jian Lu</a><sup>* 1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.rochester.edu/u/ssrivas9/" target="_blank">Shikhar Srivastava</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.rochester.edu/u/jchen175/" target="_blank">Junyu Chen</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://robikshrestha.com" target="_blank">Robik Shrestha</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.manojacharya.com" target="_blank">Manoj Acharya</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://kushalkafle.com" target="_blank">Kushal Kafle</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://chriskanan.com" target="_blank">Christopher Kanan</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>University of Rochester
              <sup>2</sup>SRI International
              <sup>3</sup>Adobe
              <!--<br>Neurips Workshop 2024</span>-->
              <span class="eql-cntrb"><small><br><sup>*</sup>jlu59@u.rochester.edu</small></span>
            </span>
          </div>
        </div>
        </div>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/KevinLuJian/MLLM_supplemental" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <img class="rounded" src="static/images/banner/banner_output_high_res.png" style="display: block; margin-left: auto; margin-right: auto; width: 100%;">
  </div>
</section>

<!-- Main Contributions
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered"></div>
      <h2 class="title is-3">Main Contributions</h2>
      <div class="content">
        <ul>
          <li> We create new 'slim' versions of the datasets suitable for zero-shot MLLM evaluation, and they are integrated into the widely-used LAVIS toolbox, facilitating the rapid and comprehensive assessment of future MLLMs. </li>
          <li> We provide a robust evaluation of MLLMs on our VQA datasets, revealing previously unreported weaknesses via fine-grained analysis across various question types and tasks. </li>
          <li> Using VQDv1, we challenge MLLMs' visual grounding capabilities by requiring them to engage in complex visual reasoning to identify multiple objects beyond the limitations of single-object referring expression datasets. </li>
        </ul>
      </div>
    </div>
  </div>
</section> -->

<!-- Introduction -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Contributions</h2> -->
        <div class="content has-text-justified">
          <!-- <p>With the advent of multi-modal large language models (MLLMs), datasets used for visual question answering (VQA) and referring expression comprehension have seen a resurgence. However, the most popular datasets used to evaluate MLLMs are some of the earliest ones created, and they have many known problems, including extreme bias, spurious correlations, and an inability to permit fine-grained analysis.</p> -->
          <p>In this paper, we pioneer evaluating recent MLLMs (LLaVA 1.5, LLaVA-NeXT, BLIP2, InstructBLIP, GPT-4V, and GPT-4o) on datasets designed to address weaknesses in earlier ones.
          <br>We assess four VQA datasets:
            <ol>
              <li><a href="#tdiuc"><strong style="color: blue;">TDIUC</strong></a>, which permits fine-grained analysis on 12 question types,</li>
              <li><a href="#tallyqa"><strong style="color: blue;">TallyQA</strong></a>, which has simple and complex counting questions, and</li>
              <li><a href="#dvqa"><strong style="color: blue;">DVQA</strong></a>, which requires optical character recognition for chart understanding.</li>
              <li><a href="#vqdv1"><strong style="color: blue;">VQDv1</strong></a>, which requires identifying all image regions that satisfy a given query. </li>
            </ol>
          </p>
          <p>Our experiments reveal the weaknesses of many MLLMs that have not previously been reported. Our code is integrated into the widely used LAVIS framework for MLLM evaluation, enabling the rapid assessment of future MLLMs.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Motivations Section -->



<!--Datasets Start here-->
<section id="vqdv1" class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">VQDv1</h2>

        <div class="content has-text-left">
          <p>
            <b style="color: blue;">Description</b>: VQDv1 requires the model to produce multiple bounding boxes instead of localizing only one object, thereby testing general query detection skills. Unlike typical referring expression datasets, which assert that every query will correspond to only one bounding box, VQDv1 queries ask the model to generate an uncertain number of bounding boxes, from 0 to N, posing an additional challenge to the model.
          </p>
          <center>
            <img src="static/images/examples/VQDv1.png" alt="Example Image" width="100%"/>
          </center>
          <p>
            <b style="color: blue;">Results</b>: All models exhibit declined Recall when the number of bounding boxes increases, implying that these models demonstrate an inability to identify multiple objects that satisfy the query. 
          </p>
          <center>
            <img src="static/images/tables/vqdv1_table.png" alt="VQDv1 Result Image" style="margin-bottom: 10px;">
            <img src="static/images/graphs/vqd-precision.png" alt="VQDv1 Performance Graph" style="margin-top: 10px;" width="50%"><img src="static/images/graphs/vqd-recall.png" alt="VQDv1 Performance Graph" style="margin-top: 10px;" width="50%">
          </center>
          <br> This trend suggests that increasing the number of grounding objects challenges the models' detection capabilities, potentially due to limitations in how they interpret contextual information within visual scenes.
        </div>
      </div>
    </div>
  </div>
</section>

<section id="tallyqa" class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">TallyQA</h2>

        <div class="content has-text-left">
          <p>
            <b style="color: blue;">Description</b>: TallyQA tests models' visual grounding through counting skills. In addition to simple counting questions that the model can handle well with straightforward object detection, TallyQA also incorporates complex counting questions that demand sophisticated reasoning capabilities, such as pose estimation (e.g., "How many dogs are sitting?") and positional reasoning (e.g., "How many dogs are in front of the white building?").
          </p>
          <p>
            <center>
              <img src="static/images/examples/tallyQA.png" alt="Example Image" width="100%"/>
            </center>
          </p>
          <p>
            <b style="color: blue;">Results</b>: TallyQA composes of two parts, simple counting questions and complex counting questions. The results show that the some models that perform well on simple counting questions may struggle with complex counting questions. For instance, BLIP2 have a comparable performance with other more complex models, such as GPT-4, but it performance drop significantly in complex counting questions, which requires, in additional to simple object detection, more sophisticated reasoning capabilities, such as pose estimation and positional reasoning. This suggests the necessity of incorporating more complex counting questions in the evaluation of visual reasoning models.
          </p>
          <center>
            <img src="static/images/tables/tallqa_table.png" alt="TallyQA Result Image" style="margin-bottom: 10px;">
          </center>
          <center>
            <img src="static/images/graphs/accuracy_distribution_complex.png" alt="TallyQA Result Image" style="margin-top: 10px;" width="50%"><img src="static/images/graphs/accuracy_distribution_simple.png" alt="TallyQA Result Image" style="margin-top: 10px;" width="50%">
          </center>
          <br> The testing metrics of TallyQA is somewhat aligned with VQDv1, the ability to count multiple objects forms the foundation for grounding multiple objects in the image. Fig 8 and 9 show that most models demonstrate performance decay when counting multiple objects. This is consistent with the results in VQDv1, where models generally perform less well on multiple object detections.
        </div>
      </div>
    </div>
  </div>
</section>

<section id="tdiuc" class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">TDIUC</h2>

        <div class="content has-text-left">
          <p>
            <b style="color: blue;">Description</b>: TDIUC tests the models' versatility across 12 tasks, including object, attribute, and activity recognition, as well as overall scene understanding. The meaningful categories of question types permit fine-grain analysis of the models' abilities from different perspectives, allowing us to identify the specific strengths and weaknesses of each model.
          </p>
          <p>
            <center>
              <img src="static/images/examples/TDIUC.png" alt="Example Image" width="100%"/>
            </center>
          </p>
          <p>
            <b style="color: blue;">Results</b>: The results in TDIUC provide us the detail performance of each model on different question types. The results show that the models have different strengths and weaknesses on different question types. A few key takeaways: all models demonstrate inability in positional reasoning. LLaVANext has the highest performance in almost all question types, even surpassing previous SoTA VQA algorithm MuRel.
          </p>
          <center>
            <img src="static/images/tables/tdiuc_table.png" alt="TDIUC Result Image" style="margin-bottom: 10px;">
          </center>
        </div>
      </div>
    </div>
  </div>
</section>

<section id="dvqa" class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">DVQA</h2>

        <div class="content has-text-left">
          <p>
            <b style="color: blue;">Description</b>: DVQA requires the models to interpret and analyze visual data in chart form, testing their ability to perform OCR and handle unusual words found in charts. The charts are all synthetically generated images, which pose additional challenges different from natural images.
          </p>
          <p>
            <center>
              <img src="static/images/examples/DVQA.png" alt="Example Image" width="100%"/>
            </center>
          </p>
          <p>
            <b style="color: blue;">Results</b>: The results for DVQA show that some models, while performing well on datasets with natural images, may struggle with synthetic images. The DVQA datasets are all synthetic images. The bars are generated with random colors and styles, with many abbreviations involved. Most models especially struggle in reasoning and data retrieval questions.
          </p>
          <center>
            <img src="static/images/tables/dvqa_table.png" alt="DVQA Result Image" style="margin-bottom: 10px;">
          </center>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- License -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-12">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

</body>
</html>
